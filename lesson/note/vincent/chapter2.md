# 第二章 数学基础
## 概率论
 1. 常用概念
* 概率
* 最大似然估计
* 条件概率
* 全概率公式:对一复杂事件A的概率求解问题转化为了在不同情况下发生的简单事件的概率的求和问题。
* 贝叶斯决策理论:在不完全情报下，对部分未知的状态用主观概率估计，然后用贝叶斯公式对发生概率进行修正，最后再利用期望值和修正概率做出最优决策。
* 贝叶斯法则
* 二项式分布
* 期望:试验中每次可能结果的概率乘以其结果的总和。
* 方差:衡量随机变量或一组数据时离散程度的度量。
* 大数定律:随机变量序列的算术平均值向随机变量各数学期望的算术平均值收敛的定律。

## 信息论
 1. 基本概念
* 熵：也称为自信息，可以被视为描述**一个随机变量的不确定性的数量**。一个随机变量的熵越大， 它的不确定性越大。那么，正确估计其值的可 能性就越小。越不确定的随机变量越需要大的信息量用以确定其值。熵率：单个变量的熵占总熵的比率。
* 联合熵：实际上就是描述一对随机变量平均所需要的信息量
* 条件熵：给定随机变量 X的情况下，随机变量 Y的条件熵
* 相对熵：KL距离， D(p||q)
* 交叉熵：H(X)+D(p||q), X~p(x),	
* 困惑度：2^H(L,q)
* 互信息：I(X;Y)=H(X)-H(X|Y)
* 噪声信道模型：
$$
H(X,q)=H(X)+D(p||q)=-\sum{p(x)}\log_2{p(x)}+\sum{p(x)}\log_2{\frac{p(x)}{q(x)}}=-\sum{p(x)}\log_2{q(x)}
$$

$$
H(X|Y)=-\sum_X\sum_Yp(x,y)\log_2p(X|Y)
$$

## 实例
### 基于上下文的消歧方法
#### 1. 基于贝叶斯分类器

$$
p(s_i|C)=\frac{p(s_i)\times p(C|s_i)}{p(C)}
$$

$$
p(s_i)=\frac{N(s_i)}{N(w)}
$$

$$
s_i为多义词w的某语义,N(x)为次数
$$
 
#### 2. 基于最大熵的消歧方法
$$
求解b属于B的p(a|b)使H(p(A|B))最大
$$

$$
p^*(a|b)=\frac1{Z(b)}exp(\sum^l_{j=1}\lambda_jf_j(a,b))
$$

$$
Z(b)= {\sum}exp(\sum^l_{j=1}\lambda_jf_j(a,b))|_a
$$

表示方法：
* 位置无关：目标词周围的词形、词性或其组合构成 的集合，如取±2 窗口范围内的词形
* 位置相关：词形（-2，-1，，1，2）顺序相关

特征选择一般有三种方法:
* 从候选特征集中选择那些在训练数据中出现频次超过一定阈值的特征;
*  利用互信息作为评价尺度从候选特征集中选择满足 一定互信息要求的特征;
*  利用增量式特征选择方法(Della Pietra et al.)从候选特 征集中选择特征。(比较复杂)
最终选定 k (k > 0) 个特征，对应 k 个特征函数 f。在以 下叙述中不再区分特征和特征函数。l=k+1

获取λ参数——利用GIS(generalized interactive scaling) 算法


## 作业

1.“任意摘录一段文字，统计这段文字中所有字符的相对频率。假设这些相对频率就是这些字符的概率，请计算其分布的熵。”共53个字符

|字符|任|意|摘|录|一|段|文|字|统|计|这|中|所|有|符|的|相|对|频|率|假|设|些|就|是|概|，|请|计|算|其|分|布|熵|。|
|---|---|---|---|---|---|---|--|--|--|--|--|--|--|--|--|--|--|--|--|---|---|--|--|--|--|--|--|--|--|--|--|--|--|--|--|
|频次|1|1|1|1|1|2|2|4|1|1|3|1|1|1|2|3|2|2|2|3|1|1|2|1|1|1|2|1|1|1|1|1|1|1|2|

22个一次，9个两次，3个三次，1个四次
熵：

$$H(X)=-(\frac{22}{53} \log{\frac{1}{53} }+\frac{9}{53} \log{\frac{2}{53} }+\frac{3}{53} \log{\frac{3}{53} }+\frac{1}{53} \log{\frac{4}{53} } )=\frac{9}{53}\log{3}+\log53-\frac{26}{53}$$

2.略<br>
3.AAB,ABBB
