# 第2章 预备知识

## 2.1 概率论基本概念

### 2.1.1 概率
[详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解](https://blog.csdn.net/u011508640/article/details/72815981)
概率（probability）是从随机试验中的时间到实数域的映射函数，用以表示时间发生的可能性。如果用
P(A)作为事件A的概率，Ω是试验的样本空间，则概率函数必须满足以下三条公理：

1. 非负性 $ P(A) ≥ 0 $
2. 规范性 $P(Ω) = 1$
3. 可列可加性 $P(\bigcup_{i=0}^\infty A_i)= \sum_{i=0}^\infty P(A_i)$

### 2.1.2 最大似然估计

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果${s_1, s_2,..., s_n}$是一个试验的样本空间，在相同的情况下重复试验N次，观察到样本$s_k(1≤k≤n)$的次数为$n_N(s_k)$，那么，$s_k$在这N次试验中的相对频率为
$$ q_N(s_k)=\frac{n_N(s_k)}{N} $$
由于$\sum_{k=1}^n n_N(s_k)=N$，因此$\sum_{k=1}^{n}q_N(s_k)=1$。
　　当N越来越大时，相对频率$q_N(s_k)$就越来越接近$s_k$的频率$P(s_k)$，事实上，
$$\lim_{N\rightarrow\infty} q_N(s_k)=P(s_k)$$
因此，通常用相对频率作为频率的估计值。这种估计概率值的方法称为最大似然估计。  
**----------------------PS:-----------------**  
**似然函数**  
　　似然(likelihood)这个词其实和概率(probability)是差不多的意思，Colins字典这么解释：The likelihood of something happening is how likely it is to happen. 你把likelihood换成probability，这解释也读得通。但是在统计里面，似然函数和概率函数却是两个不同的概念（其实也很相近就是了）。
对于这个函数：
$$P(x\mid \theta)$$  
　　输入有两个：x表示一个具体的数据；θ表示模型的参数。
如果θ是已知确定的，x是变量，这个函数叫做概率函数（probably function），它描述对于不同的样本点x，其出现概率是多少。
　　如果x是是已知确定的，θ是变量，这个函数叫做似然函数（likelihood function），它描述对于不同的模型参数，出现x这个样本点的概率是多少。
　　例如，$f(x,y)=x^y$, 即$x$的$y$次方。如果$x$是已知确定的(例如$x=2$)，这就是$f(y)=2^y$, 这是指数函数。 如果$x$是已知确定的(例如$y=2$)，这就是$f(x)=x^2$，这是二次函数。同一个数学形式，从不同的变量角度观察，可以有不同的名字。  

### 2.1.4 贝叶斯法则
**贝叶斯公式** [(参考博客)](http://www.xuyankun.cn/2017/05/13/bayes/)  
举例：  
　　一种癌症，得了这个癌症的人被检测出为阳性的几率为90%，未得这种癌症的人被检测出阴性的几率为90%，而人群中得这种癌症的几率为1%，一个人被检测出阳性，问这个人得癌症的几率为多少？  
我们用$A$表示事件“测出为阳性”，用$B_1$表示“得癌症”，$B_2$表示“未得癌症”。根据题目，我们知道如下信息：  
$$P(A\mid B_1)=0.9$$  
$$P(A\mid B_2)=0.1$$  
$$P(B_1)=0.01$$  
$$P(B_2)=0.99$$  
那么我们现在想要知道的是，已知为阳性的情况下，得癌症的几率$P(B_1|A)$。  
首先看一个概率：
$$P(B_1,A)=P(B_1)\cdot P(A\mid B_1) = 0.01\times 0.9=0.009  $$
这里$P(B_1,A)$表示的是联合概率，得癌症且检测出阳性的概率是人群中得癌症的概率乘上得癌症时测出是阳性的几率，是0.009。同理可得，未得癌症且检测出阳性的概率是：
$$P(B_2,A)=P(B_2)\cdot P(A\mid B_2) = 0.99\times 0.1=0.099  $$
这个概率概率是什么意思呢？就是说如果有1000个人，检测出阳性且得癌症的人有9人，检测出阳性但是未得癌症的有99人。可见这种仪器检测出癌症不可怕，不得癌症的是绝大多数的。  
那么在检测出阳性的前提下得癌症的概率应该怎么计算呢？  
很简单，就是看被测出为阳性的这108（99+9）人中，9人和99人分别占的比例就是我们要的。也就是说我们只需要添加一个归一化因子（normalization）就可以了。所以检测出阳性得癌症的概率$P(B_1\mid A)$为$\frac{0.009}{0.009+0.099}\approx0.083$，检测出阳性未得癌症的概率为：$P(B_2\mid A)=\frac{0.099}{0.009+0.099}\approx0.917$。  
这里$P(B_1\mid A)$和$P(B_2\mid A)$中间多了一条竖线$\mid$成为了条件概率，而这个概率就是贝叶斯统计中的**后验概率**。而人群中患癌症与否的概率$P(B_1)，P(B_2)$就是**先验概率**，根据观测值(observation)，也可以称为test evidence：是否为阳性，来判断患癌症的后验概率，这就是基本的贝叶斯思想，现在我们就可以得到本题中的后验概率公式为：  
$$P(B_i\mid A)=\frac{P(B_i)\cdot P(A\mid B_i)}{P(B_1)\cdot P(A\mid B_1)+P(B_2)\cdot P(A\mid B_2)}$$

由此可得到如下的贝叶斯公式的一般形式。  
把上面癌症例题中的A变成样本（sample）$x$，把B变成参数(parameter)$\theta$，我们便得到贝叶斯公式：
$$\pi(\theta_i\mid x) = \frac{f(x\mid\theta_i)\pi(\theta_i)}{\sum_if(x\mid\theta_i)\pi(\theta_i)}$$
可以看出上面这个例子中，$B$事件的分布是离散的，所以在分母用的是求和符号$\sum$。那么如果我们的参数$\theta$的分布是连续的呢？没错，那就要用积分，于是我们得到了真正的**贝叶斯公式**：
$$\pi(\theta\mid x) = \frac{f(x\mid\theta)\pi(\theta_i)}{\int_\Theta f(x\mid\theta_i)\pi(\theta)d\theta}$$
其中$\pi$值得是参数的概率分布，$\pi(\theta)$指的是先验概率，$\pi(\theta\mid x)$指的是后验概率，$f(x\mid\theta)$指的是我们观测到的样本的分布，也就是**似然函数**(likelihood)。记住**竖线$\mid$左边的才是我们需要的**。其中积分求的区间$\Theta$指的是参数$\theta$所有可能取到的值的域，所以可以看出后验概率$\pi(\theta\mid x)$是在知道$x$的前提先在$\Theta$域内的一个关于$\theta$的概率密度分布，每一个$\theta$都有一个对应的可能性（也就是概率）

### 2.1.8 贝叶斯决策理论  
&emsp;&emsp;贝叶斯决策理论（Bayesian decision theory）是统计方法处理模式分类问题的基本理论之一。假设研究的分类问题有c个类别，各类别的状态用$w_i$表示，$i=1,2,\cdots,c$；对应于各个类别$w_i$出现的先验概率为$P(w_i)$；在特征空间已经观察到某一向量$x$，$x=[x_i, x_2, \cdots, x_d]^T$是$d$维空间上的某一点，且条件概率密度函数$p(x\mid w_i)$是已知的。那么，利用贝叶斯公式我们可以得到后延概率$P(w_i\mid x)$如下：  
$$P(w_i\mid x) = \frac{p(x\mid w_i)P(w_i)}{\sum_{j=1}^{c}p(x\mid w_j)P(w_i)}$$
基于最小错误率的贝叶斯决策规则为：
如果$P(w_i\mid x) = \max_{j=1,2,,c}P(w_j\mid x)$，那么,$x\in w_i$。  
&emsp;&emsp;如果类别只有两类时，即$c=2$，则有：  
如果$l(x) = \frac{p(x\mid w_1)}{p(x\mid w_2)}>\frac{P(w_2)}{P(w_1)}$，则$x\in w_i$，否则$x\in w_2$。其中$l(x)$为似然比，而$\frac{P(w_2)}{P(w_1)}$成为似然比阈值。

## 2.2 信息论基本概念
### 2.2.1 熵
&emsp;&emsp;如果$X$是一个离散型随机变量，取值空间为$R$，其概率分布为$p(x)=P(X=x)$，$x\in R$。那么X的熵$H(X)$定义为：  
$$H(X)=-\sum_{x\in R}p(x)log_2p(x)$$
其中，约定$0log)=0$。$H(X)$可以写为$H(p)$。由于在公式中对数以$2$为底，该公式定义的熵的单位为二进制位（比特）。通常将$log_2p(x)$简写成$logp(x)$。
&emsp;&emsp;熵又称为自信息(self-information)，可以视为描述一个随机变量的不确定性的数量。它表示信源$X$每发一个符号（不论发什么符号）所提供的平均信息量。一个随机变量的熵越大，它的不确定性越大，那么，正确估计其值的可能性就越小。越不确定的随机变量越需要大的信息量用以确定其值。  

&emsp;&emsp;在只掌握关于未知分布的部分知识情况下，符合已知知识的概率分步可能有多个，但使熵值最大的概率分步最真实地反映了时间的分步情况，因为熵定义了随机变量的不确定性，当熵最大时，随机变量最不确定，最难准确地预测其行为。也就是说，在已知部分知识的前提下，关于未知分布最合理的推断应该是符合已知知识最不确定或最大随机的推断。  

PS：最大熵模型 https://transwarpio.github.io/teaching_ml/2017/08/15/最大熵模型/   

### 2.2.2 联合熵和条件熵
如果$X,Y$是一对离散型随机变量$X,Y~p(x,y)$，$X,Y$的联合熵(joint entropy)$H(X,Y)$定义为

$$
H(X,Y)=-\sum_{x\in X}\sum_{y\in Y}p(x,y)logp(x,y)\tag{a}
$$
联合熵实际上就是描述一对随机变量平均所需要的信息量。  
&emsp;&emsp;给定随机变量$X$的情况下，随机变量$Y$的条件熵(conditional entropy)由下式定义：  
$$
\begin{aligned}
H(X\mid X)=&\sum_{x\in X}p(x)H(Y\mid X=x)\\
=&\sum_{x\in X}p(x)[-\sum_{y\in Y}p(y\mid x)logp(y\mid x)]\\
=&-\sum_{x\in X}\sum_{y\in Y}p(x,y)logp(y\mid x)    
\end{aligned}\tag{b}
$$
将式(a)中的联合概率$logp(x,y)$展开，可得  
$$
\begin{aligned}
H(X\mid X)=&-\sum_{x\in X}\sum_{y\in Y}p(x,y)log[p(x)p(y\mid x)]\\
=&-\sum_{x\in X}\sum_{y\in Y}p(x,y)[logp(x) + logp(y\mid x)]\\
=&-\sum_{x\in X}\sum_{y\in Y}p(x,y)logp(x)-\sum_{x\in X}\sum_{y\in Y}p(x,y)logp(y\mid x)\\
=&\sum_{x\in X}p(x)logp(x0 -\sum_{x\in X}\sum_{y\in Y}p(x,y)logp(y\mid x)\\
=&H(X) + H(Y\mid X)
\end{aligned}\tag{c}
$$
我们称式(c)为熵的连锁规则。推广到一般情况，有：
$$
H(X_1,X_2,\cdots,X_n)=H(X_1)+H(X_2\mid X_1)+\cdots +H(X_n\mid X_1,\cdots,X_{n-1})
$$