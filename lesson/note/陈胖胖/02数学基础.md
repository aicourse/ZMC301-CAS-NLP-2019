# ZMC 301 宗成庆_数学基础

## 1.       概率论基础

### 概率（probability）：
* 事件发生的可能性

### 最大似然估计；
* 

### 条件概率；


### 全概率公式；


### 贝叶斯决策理论；


### 贝叶斯法则；


### 二项式分布；


### 期望；


### 方差；



## 2.       信息论基础

### 熵（entropy，H（X），又称自互信息（H（X）=I（X；X）））；
* 

### 联合熵和条件熵（熵率）；
* 

### 互信息（I（X；Y），已知Y的值后X的不确定性的减少量，即随机变量Y揭示了多少关于X的信息量）；

* 

### 噪声信道模型（Shannon定理）；

* 

### 相对熵（D（p || q），又称为Kullback-Leibler（KL）距离，从信息论角度看，如果一个随机变量X的分布为p，但我们却以概率分布为q来给它编码，那么相对熵就是这样的编码比按照概率分布p的编码多使用的比特位。）；
* 

### 交叉熵：熵的大小可以作为评估语言模型好坏的一个标准；
* 我们可以把熵看作对某个事件发生所持的“惊奇”程度，“惊奇”程度越大说明该事件的发生越出乎意料。而交叉熵则是衡量了所谓的平均“惊奇”度，计算公式：H（L，m）≈ 。

### 英语的熵：
* 英语的实际熵一般比较小，因为人们在日常的谈话中其实存在很多固定的模式，而且其中很多都是我们没有意识到的。

### 混乱度：
* 在语音识别领域中，人们通常用混乱度（Perplexity）而不是交叉熵来描述一个模型的好坏。混乱度为k意味着我们在每一步中都面临着k个等概率的选择。

## 作业

* [AI COURSE](https://github.com/aicourse/ZMC301-CAS-NLP-2019/tree/master/lesson1/assignment) 
